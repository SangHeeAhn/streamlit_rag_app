import streamlit as st
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms.ollama import Ollama
from langchain.prompts import ChatPromptTemplate
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ì •ì˜
primary_color = "#1E90FF"
secondary_color = "#FF6347"
background_color = "#F5F5F5"
text_color = "#333333"
highlight_color = "#FFD700"

# ì‚¬ìš©ì ì •ì˜ CSS
st.markdown(f"""
    <style>
    .stApp {{
        background-color: {background_color};
        color: {text_color};
    }}
    .stButton>button {{
        background-color: {primary_color};
        color: white;
        border-radius: 5px;
        padding: 10px 20px;
    }}
    .stTextInput>div>div>input {{
        border: 2px solid {primary_color};
        padding: 10px;
    }}
    .response-container {{
        background-color: white;
        padding: 15px;
        border-radius: 10px;
        box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.1);
    }}
    .highlight {{
        color: {highlight_color};
        font-weight: bold;
    }}
    </style>
""", unsafe_allow_html=True)

# ì•± íƒ€ì´í‹€
st.title("ğŸ“š RAG ì‹œìŠ¤í…œ (DeepSeek R1 & Ollama ê¸°ë°˜)")

# íŒŒì¼ ì—…ë¡œë“œ
uploaded_file = st.file_uploader("ğŸ“‚ PDF íŒŒì¼ ì—…ë¡œë“œ", type="pdf")

if uploaded_file is not None:
    # íŒŒì¼ ì €ì¥
    with open("uploaded_file.pdf", "wb") as f:
        f.write(uploaded_file.getvalue())

    # PDF ë¡œë” ì´ˆê¸°í™”
    loader = PDFPlumberLoader("uploaded_file.pdf")
    docs = loader.load()

    # ë¬¸ì„œ ë¶„í•  ë° ì„ë² ë”© ìƒì„±
    text_splitter = SemanticChunker(HuggingFaceEmbeddings())
    documents = text_splitter.split_documents(docs)

    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector = FAISS.from_documents(documents, embedder)
    retriever = vector.as_retriever(search_type="similarity", search_kwargs={"k": 3})

    # LLM ì„¤ì •
    llm = Ollama(model="deepseek-r1:32b")

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜
    system_prompt = (
        "ì£¼ì–´ì§„ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”. "
        "ë‹µë³€ì„ ëª¨ë¥¼ ê²½ìš° 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤'ë¼ê³ ë§Œ ë‹µí•˜ì„¸ìš”. "
        "ë‹µë³€ì€ ê°„ê²°í•˜ê³  ëª…í™•í•´ì•¼ í•˜ë©°, ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”. "
        "ë¬¸ë§¥: {context}"
    )

    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "{input}")
        ]
    )

    # QA ì²´ì¸ ìƒì„±
    combine_docs_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, combine_docs_chain)

    # ì‚¬ìš©ì ì…ë ¥
    user_input = st.text_input("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

    if user_input:
        with st.spinner("ğŸ” ê²€ìƒ‰ ì¤‘..."):
            response = rag_chain.invoke({"input": user_input})
            answer = response.get("answer", "ì‘ë‹µì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            # ê²°ê³¼ ì¶œë ¥
            formatted_answer = f"""
            <div class="response-container">
                <p><b>ğŸ“Œ ì‘ë‹µ:</b></p>
                <p style="color:{text_color}; font-size:16px;">{answer}</p>
            </div>
            """
            st.markdown(formatted_answer, unsafe_allow_html=True)

else:
    st.write("ğŸ“‚ **PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.**")